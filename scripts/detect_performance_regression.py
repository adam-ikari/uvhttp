#!/usr/bin/env python3
"""
UVHTTP æ€§èƒ½å›å½’æ£€æµ‹è„šæœ¬

è¿™ä¸ªè„šæœ¬æ¯”è¾ƒå½“å‰æµ‹è¯•ç»“æœä¸åŸºå‡†å€¼ï¼Œæ£€æµ‹æ€§èƒ½å›å½’ã€‚
å¦‚æœæ€§èƒ½ä¸‹é™è¶…è¿‡é˜ˆå€¼ï¼Œä¼šå‘å‡ºè­¦å‘Šæˆ–å¤±è´¥ä¿¡å·ã€‚
"""

import os
import sys
import json
import yaml
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
BASELINE_FILE = PROJECT_ROOT / "config" / "performance-baseline.yml"


class PerformanceRegressionDetector:
    def __init__(self, baseline_file: Path = BASELINE_FILE):
        self.baseline_file = baseline_file
        self.baseline = self._load_baseline()
        self.current_results = {}
        self.regressions = []
        self.warnings = []
        self.improvements = []

    def _load_baseline(self) -> Dict:
        """åŠ è½½æ€§èƒ½åŸºå‡†å€¼"""
        if not self.baseline_file.exists():
            print(f"è­¦å‘Š: åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {self.baseline_file}")
            return {}

        with open(self.baseline_file, 'r') as f:
            return yaml.safe_load(f)

    def load_results(self, results_file: Path):
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        print(f"åŠ è½½æµ‹è¯•ç»“æœ: {results_file}")

        if results_file.suffix == '.csv':
            self._load_csv_results(results_file)
        elif results_file.suffix == '.json':
            self._load_json_results(results_file)
        elif results_file.suffix == '.yml' or results_file.suffix == '.yaml':
            self._load_yaml_results(results_file)
        else:
            print(f"é”™è¯¯: ä¸æ”¯æŒçš„ç»“æœæ–‡ä»¶æ ¼å¼: {results_file.suffix}")
            sys.exit(1)

        print(f"åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(self.current_results)} ä¸ªæµ‹è¯•ç»“æœ")

    def _load_csv_results(self, file: Path):
        """åŠ è½½ CSV æ ¼å¼çš„ç»“æœ"""
        import csv

        with open(file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                test_name = row.get('test_name', row.get('Endpoint', ''))
                rps = float(row.get('RPS', row.get('rps', 0)))
                if test_name and rps > 0:
                    self.current_results[test_name] = {'rps': rps}

    def _load_json_results(self, file: Path):
        """åŠ è½½ JSON æ ¼å¼çš„ç»“æœ"""
        with open(file, 'r') as f:
            self.current_results = json.load(f)

    def _load_yaml_results(self, file: Path):
        """åŠ è½½ YAML æ ¼å¼çš„ç»“æœ"""
        with open(file, 'r') as f:
            self.current_results = yaml.safe_load(f)

    def detect_regression(self):
        """æ£€æµ‹æ€§èƒ½å›å½’"""
        print("æ£€æµ‹æ€§èƒ½å›å½’...")

        if not self.baseline:
            print("è­¦å‘Š: æ²¡æœ‰åŸºå‡†å€¼ï¼Œè·³è¿‡å›å½’æ£€æµ‹")
            return

        baseline_data = self.baseline.get('baseline', {})
        thresholds = self.baseline.get('thresholds', {})

        # æ£€æŸ¥ RPS å›å½’
        self._check_rps_regression(baseline_data, thresholds)

        # æ£€æŸ¥å»¶è¿Ÿå›å½’
        self._check_latency_regression(baseline_data, thresholds)

        # æ£€æŸ¥èµ„æºä½¿ç”¨å›å½’
        self._check_resource_regression(baseline_data, thresholds)

        print("å›å½’æ£€æµ‹å®Œæˆ")

    def _check_rps_regression(self, baseline_data: Dict, thresholds: Dict):
        """æ£€æŸ¥ RPS å›å½’"""
        rps_threshold = thresholds.get('rps', {}).get('failure', 0.10)

        # æ£€æŸ¥ä½å¹¶å‘
        self._compare_metric(
            'low_concurrent',
            'rps',
            baseline_data.get('low_concurrent', {}).get('rps', 0),
            rps_threshold,
            higher_is_better=True
        )

        # æ£€æŸ¥ä¸­ç­‰å¹¶å‘
        self._compare_metric(
            'medium_concurrent',
            'rps',
            baseline_data.get('medium_concurrent', {}).get('rps', 0),
            rps_threshold,
            higher_is_better=True
        )

        # æ£€æŸ¥é«˜å¹¶å‘
        self._compare_metric(
            'high_concurrent',
            'rps',
            baseline_data.get('high_concurrent', {}).get('rps', 0),
            rps_threshold,
            higher_is_better=True
        )

        # æ£€æŸ¥æç«¯å¹¶å‘
        self._compare_metric(
            'extreme_concurrent',
            'rps',
            baseline_data.get('extreme_concurrent', {}).get('rps', 0),
            rps_threshold,
            higher_is_better=True
        )

    def _check_latency_regression(self, baseline_data: Dict, thresholds: Dict):
        """æ£€æŸ¥å»¶è¿Ÿå›å½’"""
        latency_threshold = thresholds.get('latency', {}).get('failure', 0.20)

        # æ£€æŸ¥å„å¹¶å‘çº§åˆ«çš„å»¶è¿Ÿ
        for level in ['low_concurrent', 'medium_concurrent', 'high_concurrent', 'extreme_concurrent']:
            level_data = baseline_data.get(level, {})

            # æ£€æŸ¥å¹³å‡å»¶è¿Ÿ
            self._compare_metric(
                f'{level}_avg_latency',
                'latency_avg',
                level_data.get('latency_avg', 0),
                latency_threshold,
                higher_is_better=False
            )

            # æ£€æŸ¥ P99 å»¶è¿Ÿ
            self._compare_metric(
                f'{level}_p99_latency',
                'latency_p99',
                level_data.get('latency_p99', 0),
                latency_threshold,
                higher_is_better=False
            )

    def _check_resource_regression(self, baseline_data: Dict, thresholds: Dict):
        """æ£€æŸ¥èµ„æºä½¿ç”¨å›å½’"""
        resource_threshold = thresholds.get('resources', {}).get('failure', 0.20)

        # æ£€æŸ¥ CPU ä½¿ç”¨ç‡
        self._compare_metric(
            'cpu_usage',
            'cpu_usage',
            self.baseline.get('resources', {}).get('cpu_usage', {}).get('baseline', 0),
            resource_threshold,
            higher_is_better=False
        )

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        self._compare_metric(
            'memory_usage',
            'memory_usage',
            self.baseline.get('resources', {}).get('memory_usage', {}).get('baseline', 0),
            resource_threshold,
            higher_is_better=False
        )

    def _compare_metric(
        self,
        metric_name: str,
        metric_key: str,
        baseline_value: float,
        threshold: float,
        higher_is_better: bool
    ):
        """æ¯”è¾ƒæŒ‡æ ‡å€¼"""
        if baseline_value == 0:
            return

        # æŸ¥æ‰¾å½“å‰å€¼
        current_value = self._find_current_value(metric_name, metric_key)
        if current_value is None:
            return

        # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
        if higher_is_better:
            change_percent = (baseline_value - current_value) / baseline_value
        else:
            change_percent = (current_value - baseline_value) / baseline_value

        # åˆ¤æ–­æ˜¯å¦å›å½’
        if change_percent > threshold:
            self.regressions.append({
                'metric': metric_name,
                'baseline': baseline_value,
                'current': current_value,
                'change_percent': change_percent * 100,
                'threshold': threshold * 100,
                'higher_is_better': higher_is_better
            })
        elif change_percent > threshold * 0.5:
            self.warnings.append({
                'metric': metric_name,
                'baseline': baseline_value,
                'current': current_value,
                'change_percent': change_percent * 100,
                'threshold': threshold * 100,
                'higher_is_better': higher_is_better
            })
        elif change_percent < -threshold * 0.5:
            self.improvements.append({
                'metric': metric_name,
                'baseline': baseline_value,
                'current': current_value,
                'change_percent': change_percent * 100,
                'higher_is_better': higher_is_better
            })

    def _find_current_value(self, metric_name: str, metric_key: str) -> Optional[float]:
        """æŸ¥æ‰¾å½“å‰å€¼"""
        # å°è¯•ç›´æ¥åŒ¹é…
        if metric_name in self.current_results:
            if metric_key in self.current_results[metric_name]:
                return float(self.current_results[metric_name][metric_key])

        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        for key, value in self.current_results.items():
            if metric_key in value:
                return float(value[metric_key])

        return None

    def generate_report(self, output_file: Optional[Path] = None):
        """ç”Ÿæˆå›å½’æ£€æµ‹æŠ¥å‘Š"""
        print("ç”Ÿæˆå›å½’æ£€æµ‹æŠ¥å‘Š...")

        report = {
            'timestamp': datetime.now().isoformat(),
            'baseline_file': str(self.baseline_file),
            'regressions': self.regressions,
            'warnings': self.warnings,
            'improvements': self.improvements,
            'summary': self._generate_summary()
        }

        # æ‰“å°æŠ¥å‘Š
        self._print_report(report)

        # ä¿å­˜æŠ¥å‘Š
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(report, f, indent=2)
            print(f"æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

        return report

    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆæ‘˜è¦"""
        return {
            'total_regressions': len(self.regressions),
            'total_warnings': len(self.warnings),
            'total_improvements': len(self.improvements),
            'has_regression': len(self.regressions) > 0,
            'has_warning': len(self.warnings) > 0,
            'has_improvement': len(self.improvements) > 0,
            'status': self._get_status()
        }

    def _get_status(self) -> str:
        """è·å–çŠ¶æ€"""
        if len(self.regressions) > 0:
            return 'failure'
        elif len(self.warnings) > 0:
            return 'warning'
        elif len(self.improvements) > 0:
            return 'improvement'
        else:
            return 'pass'

    def _print_report(self, report: Dict):
        """æ‰“å°æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("æ€§èƒ½å›å½’æ£€æµ‹æŠ¥å‘Š")
        print("=" * 80)
        print(f"æ£€æµ‹æ—¶é—´: {report['timestamp']}")
        print(f"åŸºå‡†æ–‡ä»¶: {report['baseline_file']}")
        print()

        # æ‰“å°å›å½’
        if report['summary']['total_regressions'] > 0:
            print(" æ€§èƒ½å›å½’ (éœ€è¦ä¿®å¤):")
            print("-" * 80)
            for regression in report['regressions']:
                direction = "ä¸‹é™" if regression['higher_is_better'] else "å¢åŠ "
                print(f"  æŒ‡æ ‡: {regression['metric']}")
                print(f"  åŸºå‡†å€¼: {regression['baseline']}")
                print(f"  å½“å‰å€¼: {regression['current']}")
                print(f"  å˜åŒ–: {regression['change_percent']:.2f}% ({direction})")
                print(f"  é˜ˆå€¼: {regression['threshold']:.2f}%")
                print()
        else:
            print(" æœªæ£€æµ‹åˆ°æ€§èƒ½å›å½’")

        # æ‰“å°è­¦å‘Š
        if report['summary']['total_warnings'] > 0:
            print("\n  æ€§èƒ½è­¦å‘Š (éœ€è¦å…³æ³¨):")
            print("-" * 80)
            for warning in report['warnings']:
                direction = "ä¸‹é™" if warning['higher_is_better'] else "å¢åŠ "
                print(f"  æŒ‡æ ‡: {warning['metric']}")
                print(f"  åŸºå‡†å€¼: {warning['baseline']}")
                print(f"  å½“å‰å€¼: {warning['current']}")
                print(f"  å˜åŒ–: {warning['change_percent']:.2f}% ({direction})")
                print(f"  é˜ˆå€¼: {warning['threshold']:.2f}%")
                print()

        # æ‰“å°æ”¹è¿›
        if report['summary']['total_improvements'] > 0:
            print("\nğŸ‰ æ€§èƒ½æ”¹è¿›:")
            print("-" * 80)
            for improvement in report['improvements']:
                direction = "æå‡" if improvement['higher_is_better'] else "é™ä½"
                print(f"  æŒ‡æ ‡: {improvement['metric']}")
                print(f"  åŸºå‡†å€¼: {improvement['baseline']}")
                print(f"  å½“å‰å€¼: {improvement['current']}")
                print(f"  å˜åŒ–: {abs(improvement['change_percent']):.2f}% ({direction})")
                print()

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 80)
        print("æ‘˜è¦")
        print("=" * 80)
        print(f"å›å½’æ•°é‡: {report['summary']['total_regressions']}")
        print(f"è­¦å‘Šæ•°é‡: {report['summary']['total_warnings']}")
        print(f"æ”¹è¿›æ•°é‡: {report['summary']['total_improvements']}")
        print(f"çŠ¶æ€: {report['summary']['status'].upper()}")
        print("=" * 80)

    def exit_with_status(self):
        """æ ¹æ®æ£€æµ‹ç»“æœé€€å‡º"""
        summary = self._generate_summary()

        if summary['has_regression']:
            print("\n æ£€æµ‹åˆ°æ€§èƒ½å›å½’ï¼Œé€€å‡ºç : 1")
            sys.exit(1)
        elif summary['has_warning']:
            print("\n  æ£€æµ‹åˆ°æ€§èƒ½è­¦å‘Šï¼Œé€€å‡ºç : 2")
            sys.exit(2)
        else:
            print("\n æ€§èƒ½æ£€æµ‹é€šè¿‡ï¼Œé€€å‡ºç : 0")
            sys.exit(0)


def main():
    parser = argparse.ArgumentParser(description='UVHTTP æ€§èƒ½å›å½’æ£€æµ‹')
    parser.add_argument('results_file', type=Path, help='æµ‹è¯•ç»“æœæ–‡ä»¶ (CSV/JSON/YAML)')
    parser.add_argument('--baseline', type=Path, default=BASELINE_FILE,
                        help='åŸºå‡†æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/performance-baseline.yml)')
    parser.add_argument('--output', type=Path, help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--fail-on-regression', action='store_true',
                        help='æ£€æµ‹åˆ°å›å½’æ—¶é€€å‡ºç ä¸º 1')

    args = parser.parse_args()

    # åˆ›å»ºæ£€æµ‹å™¨
    detector = PerformanceRegressionDetector(args.baseline)

    # åŠ è½½ç»“æœ
    detector.load_results(args.results_file)

    # æ£€æµ‹å›å½’
    detector.detect_regression()

    # ç”ŸæˆæŠ¥å‘Š
    detector.generate_report(args.output)

    # æ ¹æ®æ£€æµ‹ç»“æœé€€å‡º
    if args.fail_on_regression:
        detector.exit_with_status()


if __name__ == "__main__":
    main()