#!/usr/bin/env python3
"""
PR é€šçŸ¥è„šæœ¬
ç”¨äºåœ¨ PR ä¸­æ·»åŠ  CI/CD ç»“æœè¯„è®º
"""

import os
import sys
import json
import argparse


def load_performance_results(results_file):
    """åŠ è½½æ€§èƒ½æµ‹è¯•ç»“æœ"""
    try:
        with open(results_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading performance results: {e}", file=sys.stderr)
        return None


def load_baseline(baseline_file):
    """åŠ è½½æ€§èƒ½åŸºçº¿"""
    try:
        with open(baseline_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading baseline: {e}", file=sys.stderr)
        return None


def compare_performance(current, baseline):
    """æ¯”è¾ƒå½“å‰æ€§èƒ½ä¸åŸºçº¿"""
    results = {
        'regression': False,
        'improvement': False,
        'details': []
    }
    
    if not current or not baseline:
        return results
    
    # è·å– RPS æ•°æ®
    current_rps = None
    baseline_rps = None
    
    if 'test_scenarios' in current:
        for scenario in current['test_scenarios']:
            if 'results' in scenario and 'rps' in scenario['results']:
                current_rps = scenario['results']['rps']['value']
                break
    
    if 'baseline' in baseline and 'rps' in baseline['baseline']:
        baseline_rps = baseline['baseline']['rps']
    
    if current_rps and baseline_rps:
        change = ((current_rps - baseline_rps) / baseline_rps) * 100
        
        results['details'].append({
            'metric': 'RPS',
            'baseline': baseline_rps,
            'current': current_rps,
            'change': change
        })
        
        # å›å½’é˜ˆå€¼ï¼šRPS ä¸‹é™è¶…è¿‡ 10%
        if change < -10:
            results['regression'] = True
        # æ”¹è¿›é˜ˆå€¼ï¼šRPS æå‡è¶…è¿‡ 10%
        elif change > 10:
            results['improvement'] = True
    
    return results


def generate_pr_comment(workflow_name, conclusion, run_id, run_number, 
                       performance_comparison=None, test_results=None):
    """ç”Ÿæˆ PR è¯„è®ºå†…å®¹"""
    comment = f"## ğŸ¤– CI/CD PR Validation Results\n\n"
    comment += f"**Workflow**: {workflow_name}\n"
    comment += f"**Run**: #{run_number}\n"
    
    # çŠ¶æ€å›¾æ ‡
    if conclusion == 'success':
        comment += f"**Status**: âœ… Passed\n\n"
    else:
        comment += f"**Status**: âŒ Failed\n\n"
    
    # æ€§èƒ½æ¯”è¾ƒ
    if performance_comparison:
        comment += "### ğŸ“Š Performance Comparison\n\n"
        
        if performance_comparison['regression']:
            comment += "âš ï¸ **Performance Regression Detected**\n\n"
        elif performance_comparison['improvement']:
            comment += "âœ¨ **Performance Improvement Detected**\n\n"
        else:
            comment += "âœ… **Performance Stable**\n\n"
        
        comment += "| Metric | Baseline | Current | Change |\n"
        comment += "|--------|----------|---------|--------|\n"
        
        for detail in performance_comparison['details']:
            change_pct = detail['change']
            change_str = f"{change_pct:+.2f}%"
            
            if change_pct < -10:
                change_str = f"ğŸ”´ {change_str}"
            elif change_pct > 10:
                change_str = f"ğŸŸ¢ {change_str}"
            
            comment += f"| {detail['metric']} | {detail['baseline']:.2f} | {detail['current']:.2f} | {change_str} |\n"
        
        comment += "\n"
    
    # æµ‹è¯•ç»“æœ
    if test_results:
        comment += "### ğŸ§ª Test Results\n\n"
        comment += f"- Total: {test_results.get('total', 0)}\n"
        comment += f"- Passed: {test_results.get('passed', 0)} âœ…\n"
        comment += f"- Failed: {test_results.get('failed', 0)} âŒ\n"
        comment += f"- Skipped: {test_results.get('skipped', 0)} â­ï¸\n\n"
    
    # è¯¦ç»†é“¾æ¥
    comment += "### ğŸ”— Links\n\n"
    comment += f"- [View Workflow Run](https://github.com/adam-ikari/uvhttp/actions/runs/{run_id})\n"
    
    if conclusion == 'success':
        comment += "\nReady for review! ğŸ‰"
    else:
        comment += "\nPlease fix the issues and push a new commit."
    
    return comment


def main():
    parser = argparse.ArgumentParser(description='Generate PR notification comment')
    parser.add_argument('--workflow-name', required=True, help='Workflow name')
    parser.add_argument('--conclusion', required=True, choices=['success', 'failure', 'cancelled', 'skipped'], help='Workflow conclusion')
    parser.add_argument('--run-id', required=True, help='Workflow run ID')
    parser.add_argument('--run-number', required=True, help='Workflow run number')
    parser.add_argument('--performance-results', help='Path to performance results JSON')
    parser.add_argument('--baseline', help='Path to baseline JSON')
    parser.add_argument('--test-results', help='Path to test results JSON')
    parser.add_argument('--output', '-o', help='Output file (default: stdout)')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    performance_data = None
    baseline_data = None
    test_data = None
    
    if args.performance_results:
        performance_data = load_performance_results(args.performance_results)
    
    if args.baseline:
        baseline_data = load_baseline(args.baseline)
    
    if args.test_results:
        with open(args.test_results, 'r') as f:
            test_data = json.load(f)
    
    # æ¯”è¾ƒæ€§èƒ½
    performance_comparison = None
    if performance_data and baseline_data:
        performance_comparison = compare_performance(performance_data, baseline_data)
    
    # ç”Ÿæˆè¯„è®º
    comment = generate_pr_comment(
        args.workflow_name,
        args.conclusion,
        args.run_id,
        args.run_number,
        performance_comparison,
        test_data
    )
    
    # è¾“å‡º
    if args.output:
        with open(args.output, 'w') as f:
            f.write(comment)
        print(f"Comment written to {args.output}")
    else:
        print(comment)


if __name__ == '__main__':
    main()