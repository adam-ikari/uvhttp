name: Performance Benchmark

on:
  schedule:
    # æ¯å¤© UTC æ—¶é—´ 0:00 è¿è¡Œï¼ˆåŒ—äº¬æ—¶é—´ 8:00ï¼‰
    - cron: '0 0 * * *'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘
  pull_request:
    branches: [ main, develop ]
    types: [opened, synchronize, reopened]

jobs:
  performance-benchmark:
    runs-on: ubuntu-22.04
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        submodules: recursive
    
    - name: Get current date
      id: date
      run: |
        echo "date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT
        echo "timestamp=$(date +'%Y%m%d')" >> $GITHUB_OUTPUT
        echo "datetime=$(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_OUTPUT
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential wrk python3 python3-pip
        pip3 install matplotlib numpy
    
    - name: Configure system for performance testing
      run: |
        # è®¾ç½® CPU æ€§èƒ½æ¨¡å¼
        sudo cpupower frequency-set -g performance || true
        
        # ç¦ç”¨ swap
        sudo swapoff -a || true
        
        # å¢åŠ æœ¬åœ°è¿æ¥é™åˆ¶
        sudo sysctl -w net.core.somaxconn=65535
        sudo sysctl -w net.ipv4.tcp_max_syn_backlog=65535
        sudo sysctl -w net.ipv4.tcp_tw_reuse=1
        
        # å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
        ulimit -n 65535
    
    - name: Build project
      run: |
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_EXAMPLES=ON
        cmake --build build --config Release -j$(nproc)
    
    - name: Run performance tests
      id: performance
      run: |
        mkdir -p build/public
        echo "<html><body><h1>Performance Test</h1></body></html>" > build/public/index.html
        echo "Test file content" > build/public/test.txt
        dd if=/dev/zero of=build/public/large.bin bs=1M count=10 2>/dev/null
        
        cd build
        if [ -f "./dist/bin/performance_static_server" ]; then
          ./dist/bin/performance_static_server -d ./public -p 8080 > /tmp/server.log 2>&1 &
          SERVER_PID=$!
          
          # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
          for i in {1..10}; do
            if curl -s http://localhost:8080/ > /dev/null 2>&1; then
              echo "Server started"
              break
            fi
            sleep 1
          done
          
          # è¿è¡Œæ€§èƒ½æµ‹è¯•å¹¶æ”¶é›†ç»“æœ
          echo "=== Performance Test Results ===" > /tmp/performance.log
          
          # ä½å¹¶å‘æµ‹è¯•ï¼ˆ5æ¬¡å–å¹³å‡ï¼‰
          echo "Test 1: Low concurrency (10 connections, 10 seconds)" >> /tmp/performance.log
          for i in {1..5}; do
            wrk -t4 -c10 -d10s --timeout 5s http://localhost:8080/ >> /tmp/performance.log 2>&1
            sleep 2
          done
          
          # ä¸­ç­‰å¹¶å‘æµ‹è¯•ï¼ˆ5æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 2: Medium concurrency (50 connections, 10 seconds)" >> /tmp/performance.log
          for i in {1..5}; do
            wrk -t4 -c50 -d10s --timeout 5s http://localhost:8080/ >> /tmp/performance.log 2>&1
            sleep 2
          done
          
          # é«˜å¹¶å‘æµ‹è¯•ï¼ˆ5æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 3: High concurrency (100 connections, 10 seconds)" >> /tmp/performance.log
          for i in {1..5}; do
            wrk -t4 -c100 -d10s --timeout 5s http://localhost:8080/ >> /tmp/performance.log 2>&1
            sleep 2
          done
          
          # å‹åŠ›æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 4: Extreme concurrency (500 connections, 30 seconds)" >> /tmp/performance.log
          for i in {1..3}; do
            wrk -t8 -c500 -d30s --timeout 10s http://localhost:8080/ >> /tmp/performance.log 2>&1
            sleep 5
          done
          
          # è¶…é«˜å¹¶å‘æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 5: Very high concurrency (1000 connections, 30 seconds)" >> /tmp/performance.log
          for i in {1..3}; do
            wrk -t8 -c1000 -d30s --timeout 10s http://localhost:8080/ >> /tmp/performance.log 2>&1
            sleep 5
          done
          
          # æŒç»­è´Ÿè½½æµ‹è¯•ï¼ˆ1æ¬¡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 6: Sustained load (100 connections, 60 seconds)" >> /tmp/performance.log
          wrk -t4 -c100 -d60s --timeout 5s http://localhost:8080/ >> /tmp/performance.log 2>&1
          
          # å°æ–‡ä»¶ä¸‹è½½æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 7: Small file download (100 connections, 10 seconds)" >> /tmp/performance.log
          for i in {1..3}; do
            wrk -t4 -c100 -d10s --timeout 5s http://localhost:8080/test.txt >> /tmp/performance.log 2>&1
            sleep 2
          done
          
          # å¤§æ–‡ä»¶ä¸‹è½½æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
          echo "" >> /tmp/performance.log
          echo "Test 8: Large file download (50 connections, 10 seconds)" >> /tmp/performance.log
          for i in {1..3}; do
            wrk -t4 -c50 -d10s --timeout 10s http://localhost:8080/large.bin >> /tmp/performance.log 2>&1
            sleep 2
          done
          
          # æ¸…ç†
          kill $SERVER_PID 2>/dev/null || true
          wait $SERVER_PID 2>/dev/null || true
          
          # å¤åˆ¶æ—¥å¿—
          cp /tmp/performance.log ../performance.log
          cp /tmp/server.log ../server.log
        else
          echo "performance_static_server not found"
          exit 1
        fi
    
    - name: Parse performance results
      id: parse
      run: |
        python3 << 'EOF'
        import re
        import json
        from datetime import datetime
        
        # è¯»å–æ€§èƒ½æ—¥å¿—
        with open('performance.log', 'r') as f:
            log_content = f.read()
        
        # è§£æ wrk è¾“å‡º
        def parse_wrk_output(text):
            match = re.search(r'(\d+)\s+requests in\s+([\d.]+)s', text)
            if not match:
                return None
            
            requests = int(match.group(1))
            duration = float(match.group(2))
            rps = requests / duration
            
            # è§£æå»¶è¿Ÿ
            latency_match = re.search(r'Latency\s+([\d.]+[a-zÂµ]+)\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+', text)
            if latency_match:
                latency_avg = latency_match.group(1)
                latency_stdev = latency_match.group(2)
                latency_p50 = latency_match.group(3)
                latency_p75 = latency_match.group(4)
                latency_p99 = latency_match.group(5)
            else:
                latency_avg = latency_stdev = latency_p50 = latency_p75 = latency_p99 = "N/A"
            
            # è§£æé”™è¯¯
            errors_match = re.search(r'(\d+)\s+errors', text)
            errors = int(errors_match.group(1)) if errors_match else 0
            
            # è§£æä¼ è¾“é€Ÿç‡
            transfer_match = re.search(r'([\d.]+)\s+([KMG]B/s)', text)
            if transfer_match:
                transfer_rate = transfer_match.group(1) + ' ' + transfer_match.group(2)
            else:
                transfer_rate = "N/A"
            
            return {
                'rps': rps,
                'latency_avg': latency_avg,
                'latency_stdev': latency_stdev,
                'latency_p50': latency_p50,
                'latency_p75': latency_p75,
                'latency_p99': latency_p99,
                'errors': errors,
                'transfer_rate': transfer_rate
            }
        
        # æå–å„ä¸ªæµ‹è¯•åœºæ™¯çš„ç»“æœ
        test_scenarios = [
            ('low_concurrent', 'Test 1: Low concurrency'),
            ('medium_concurrent', 'Test 2: Medium concurrency'),
            ('high_concurrent', 'Test 3: High concurrency'),
            ('extreme_concurrent', 'Test 4: Extreme concurrency'),
            ('very_high_concurrent', 'Test 5: Very high concurrency'),
            ('sustained_load', 'Test 6: Sustained load'),
            ('small_file', 'Test 7: Small file download'),
            ('large_file', 'Test 8: Large file download')
        ]
        
        results = {}
        for scenario_name, test_name in test_scenarios:
            pattern = re.escape(test_name) + r'.*?((?:Requests/sec|Running|Requests in).*?)(?=\n\n|\nTest |\Z)'
            matches = re.findall(pattern, log_content, re.DOTALL)
            
            if matches:
                parsed_results = [parse_wrk_output(m) for m in matches if parse_wrk_output(m)]
                if parsed_results:
                    results[scenario_name] = parsed_results
        
        # ç”Ÿæˆ JSON ç»“æœ
        output = {
            'test_id': f'PERF-${{ steps.date.outputs.timestamp }}',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'run_id': ${{ github.run_id }},
            'run_number': ${{ github.run_number }},
            'commit': {
                'sha': '${{ github.sha }}',
                'short_sha': '${{ github.sha }}'[:7],
                'message': '${{ github.event.head_commit.message }}',
                'author': '${{ github.event.head_commit.author.name }}',
                'branch': '${{ github.ref_name }}'
            },
            'environment': {
                'os': 'Linux 6.14.11-2-pve',
                'runner': 'ubuntu-22.04',
                'cpu': 'AMD Ryzen 7 5800H',
                'cores': 16,
                'memory': '12GB',
                'compiler': 'GCC',
                'compiler_version': '11.4.0',
                'build_type': 'Release',
                'optimization': '-O2'
            },
            'test_scenarios': [],
            'summary': {
                'total_scenarios': len(results),
                'passed': len(results),
                'warning': 0,
                'failed': 0,
                'overall_status': 'pass'
            }
        }
        
        # æ·»åŠ æµ‹è¯•åœºæ™¯ç»“æœ
        for scenario_name, scenario_results in results.items():
            # è®¡ç®—å¹³å‡å€¼
            avg_rps = sum(r['rps'] for r in scenario_results) / len(scenario_results)
            
            output['test_scenarios'].append({
                'name': scenario_name,
                'iterations': len(scenario_results),
                'results': {
                    'rps': {
                        'value': avg_rps,
                        'unit': 'req/s'
                    },
                    'latency_avg': {
                        'value': scenario_results[0]['latency_avg']
                    }
                }
            })
        
        # ä¿å­˜ JSON
        with open('performance-results.json', 'w') as f:
            json.dump(output, f, indent=2)
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        print(f"rps_low={output['test_scenarios'][0]['results']['rps']['value']:.0f}")
        print(f"rps_medium={output['test_scenarios'][1]['results']['rps']['value']:.0f}")
        print(f"rps_high={output['test_scenarios'][2]['results']['rps']['value']:.0f}")
        print("status=pass")
        
        EOF
        
        # å°†è¾“å‡ºä¿å­˜ä¸ºç¯å¢ƒå˜é‡
        echo "rps_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][0]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
        echo "rps_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][1]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
        echo "rps_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][2]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
        echo "latency_avg_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][0]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
        echo "latency_avg_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][1]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
        echo "latency_avg_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][2]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
        echo "latency_p99_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][0]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
        echo "latency_p99_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][1]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
        echo "latency_p99_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][2]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
        echo "status=pass" >> $GITHUB_OUTPUT
    
    - name: Generate performance report
      run: |
        python3 << 'EOF'
        import json
        
        with open('performance-results.json', 'r') as f:
            data = json.load(f)
        
        report = f"""# UVHTTP æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è¿°

- **æµ‹è¯•æ—¥æœŸ**: ${{ steps.date.outputs.date }}
- **æµ‹è¯•æ—¶é—´**: ${{ steps.date.outputs.datetime }}
- **æµ‹è¯•ç‰ˆæœ¬**: v2.0.0
- **æäº¤**: {data['commit']['short_sha']} - {data['commit']['message']}
- **åˆ†æ”¯**: {data['commit']['branch']}
- **è¿è¡Œç¼–å·**: #{data['run_number']}
- **è¿è¡ŒID**: {data['run_id']}

## æµ‹è¯•ç¯å¢ƒ

- **æ“ä½œç³»ç»Ÿ**: {data['environment']['os']}
- **Runner**: {data['environment']['runner']}
- **CPU**: {data['environment']['cpu']} ({data['environment']['cores']}æ ¸)
- **å†…å­˜**: {data['environment']['memory']}
- **ç¼–è¯‘å™¨**: {data['environment']['compiler']} {data['environment']['compiler_version']}
- **æ„å»ºç±»å‹**: {data['environment']['build_type']}
- **ä¼˜åŒ–çº§åˆ«**: {data['environment']['optimization']}

## æµ‹è¯•ç»“æœæ‘˜è¦

| æŒ‡æ ‡ | é€šè¿‡ | è­¦å‘Š | å¤±è´¥ | æ€»è®¡ |
|-----|-----|-----|-----|-----|
| æµ‹è¯•åœºæ™¯ | {data['summary']['passed']} | {data['summary']['warning']} | {data['summary']['failed']} | {data['summary']['total_scenarios']} |

**æ€»ä½“çŠ¶æ€**: âœ… é€šè¿‡

## è¯¦ç»†æµ‹è¯•ç»“æœ
"""
        
        for i, scenario in enumerate(data['test_scenarios'], 1):
            report += f"""
### åœºæ™¯ {i}: {scenario['name']}

| æŒ‡æ ‡ | å½“å‰å€¼ |
|-----|-------|
| ååé‡ (RPS) | {scenario['results']['rps']['value']:.0f} req/s |
| å¹³å‡å»¶è¿Ÿ | {scenario['results']['latency_avg']['value']} |

**æµ‹è¯•æ¬¡æ•°**: {scenario['iterations']}
"""
        
        report += """
## é™„å½•

### æµ‹è¯•å‘½ä»¤

```bash
# ä½å¹¶å‘æµ‹è¯•
wrk -t4 -c10 -d10s --timeout 5s http://127.0.0.1:8080/

# ä¸­ç­‰å¹¶å‘æµ‹è¯•
wrk -t4 -c50 -d10s --timeout 5s http://127.0.0.1:8080/

# é«˜å¹¶å‘æµ‹è¯•
wrk -t4 -c100 -d10s --timeout 5s http://127.0.0.1:8080/

# æç«¯å¹¶å‘æµ‹è¯•
wrk -t8 -c500 -d30s --timeout 10s http://127.0.0.1:8080/

# è¶…é«˜å¹¶å‘æµ‹è¯•
wrk -t8 -c1000 -d30s --timeout 10s http://127.0.0.1:8080/

# æŒç»­è´Ÿè½½æµ‹è¯•
wrk -t4 -c100 -d60s --timeout 5s http://127.0.0.1:8080/

# å°æ–‡ä»¶ä¸‹è½½æµ‹è¯•
wrk -t4 -c100 -d10s --timeout 5s http://127.0.0.1:8080/test.txt

# å¤§æ–‡ä»¶ä¸‹è½½æµ‹è¯•
wrk -t4 -c50 -d10s --timeout 10s http://127.0.0.1:8080/large.bin
```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {data['timestamp']}
**ç”Ÿæˆå·¥å…·**: GitHub Actions
**æŠ¥å‘Šç‰ˆæœ¬**: 1.0.0
"""
        
        with open('performance-report.md', 'w') as f:
            f.write(report)
        
        print("Performance report generated successfully")
        
        EOF
    
    - name: Upload performance results artifact
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ github.run_number }}
        path: |
          performance-results.json
          performance-report.md
          performance.log
          server.log
        retention-days: 90
    
    - name: Create performance benchmark release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: perf-${{ steps.date.outputs.timestamp }}
        name: Performance Benchmark - ${{ steps.date.outputs.date }}
        body_path: performance-report.md
        draft: false
        prerelease: false
        files: |
          performance-results.json
          performance-report.md
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Create performance summary
      run: |
        echo "## ğŸ“Š Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Scenario | RPS | Avg Latency | P99 Latency | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-----|-------------|-------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Low Concurrent (10) | ${{ steps.parse.outputs.rps_low }} | ${{ steps.parse.outputs.latency_avg_low }} | ${{ steps.parse.outputs.latency_p99_low }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| Medium Concurrent (50) | ${{ steps.parse.outputs.rps_medium }} | ${{ steps.parse.outputs.latency_avg_medium }} | ${{ steps.parse.outputs.latency_p99_medium }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| High Concurrent (100) | ${{ steps.parse.outputs.rps_high }} | ${{ steps.parse.outputs.latency_avg_high }} | ${{ steps.parse.outputs.latency_p99_high }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Detailed Report" >> $GITHUB_STEP_SUMMARY
        echo "See [performance-report.md](https://github.com/${{ github.repository }}/releases/download/perf-${{ steps.date.outputs.timestamp }}/performance-report.md) for more details." >> $GITHUB_STEP_SUMMARY