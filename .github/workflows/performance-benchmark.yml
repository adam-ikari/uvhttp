name: CI/CD - Performance Benchmark

on:
  schedule:
    # æ¯å¤© UTC æ—¶é—´ 0:00 è¿è¡Œï¼ˆåŒ—äº¬æ—¶é—´ 8:00ï¼‰
    - cron: '0 0 * * *'
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘
  pull_request:
    branches: [ main, develop ]
    types: [opened, synchronize, reopened]

env:
  # åˆ¤æ–­æ˜¯å¦ä¸º PR è§¦å‘
  IS_PR: ${{ github.event_name == 'pull_request' }}

jobs:
  performance-benchmark:
    runs-on: ubuntu-22.04
    # è¶…æ—¶è®¾ç½®è¯´æ˜ï¼š
    # - PR å¿«é€Ÿæµ‹è¯•ï¼š5åˆ†é’Ÿ = 3ä¸ªæµ‹è¯•Ã—5ç§’ + 3ç§’å¯åŠ¨ + 2ç§’æ¸…ç† + å®‰å…¨ä½™é‡
    # - å®Œæ•´æµ‹è¯•ï¼š30åˆ†é’Ÿ = 3ä¸ªåœºæ™¯Ã—å¤šæ¬¡è¿­ä»£ + åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
    # - å®é™…æµ‹è¯•æ—¶é—´ï¼šPRçº¦20ç§’ï¼Œå®Œæ•´æµ‹è¯•çº¦5-10åˆ†é’Ÿ
    timeout-minutes: ${{ github.event_name == 'pull_request' && 5 || 30 }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4.1.1
      with:
        submodules: recursive
    
    - name: Get current date
      id: date
      run: |
        echo "date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT
        echo "timestamp=$(date +'%Y%m%d')" >> $GITHUB_OUTPUT
        echo "datetime=$(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_OUTPUT
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential wrk python3 python3-pip
        pip3 install matplotlib numpy
    
    - name: Configure system for performance testing
      run: |
        # è®¾ç½® CPU æ€§èƒ½æ¨¡å¼
        sudo cpupower frequency-set -g performance || true
        
        # ç¦ç”¨ swap
        sudo swapoff -a || true
        
        # å¢åŠ æœ¬åœ°è¿æ¥é™åˆ¶
        sudo sysctl -w net.core.somaxconn=65535
        sudo sysctl -w net.ipv4.tcp_max_syn_backlog=65535
        sudo sysctl -w net.ipv4.tcp_tw_reuse=1
        
        # å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
        ulimit -n 65535
    
    - name: Build project
      run: |
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_EXAMPLES=ON
        cmake --build build --config Release -j$(nproc)
    
    - name: Run performance tests
      id: performance
      env:
        IS_PR: ${{ github.event_name == 'pull_request' }}
      run: |
        cd build
        if [ -f "./dist/bin/benchmark_rps" ]; then
          chmod +x ./dist/bin/benchmark_rps
          ./dist/bin/benchmark_rps > /tmp/server.log 2>&1 &
          SERVER_PID=$!
          
          # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
          for i in {1..10}; do
            if curl -s http://localhost:18081/ > /dev/null 2>&1; then
              echo "Server started"
              break
            fi
            sleep 1
          done
          
          # è¿è¡Œæ€§èƒ½æµ‹è¯•å¹¶æ”¶é›†ç»“æœ
          echo "=== Performance Test Results ===" > /tmp/performance.log
          
          if [ "$IS_PR" = "true" ]; then
            echo "Running quick performance tests for PR..."
            
            # PR å¿«é€Ÿæµ‹è¯•è¯´æ˜ï¼š
            # - æµ‹è¯•æ—¶é•¿ï¼š5ç§’ï¼ˆæƒè¡¡é€Ÿåº¦ä¸å‡†ç¡®æ€§ï¼Œè¶³å¤Ÿæ£€æµ‹æ€§èƒ½å›å½’ï¼‰
            # - æµ‹è¯•æ¬¡æ•°ï¼š1æ¬¡ï¼ˆPR å¿«é€ŸéªŒè¯ï¼Œè¯¦ç»†æµ‹è¯•åœ¨å®šæ—¶ä»»åŠ¡ä¸­ï¼‰
            # - æ€»è€—æ—¶ï¼šçº¦15ç§’ï¼ˆ3ä¸ªæµ‹è¯•Ã—5ç§’ + å¯åŠ¨/æ¸…ç†å¼€é”€ï¼‰
            # - ç›®çš„ï¼šå¿«é€ŸéªŒè¯ PR æ˜¯å¦å¼•å…¥æ€§èƒ½å›å½’
            echo "Test 1: Low concurrency (10 connections, 5 seconds)" >> /tmp/performance.log
            wrk -t2 -c10 -d5s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
            
            echo "" >> /tmp/performance.log
            echo "Test 2: Medium concurrency (50 connections, 5 seconds)" >> /tmp/performance.log
            wrk -t4 -c50 -d5s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
            
            echo "" >> /tmp/performance.log
            echo "Test 3: High concurrency (200 connections, 5 seconds)" >> /tmp/performance.log
            wrk -t8 -c200 -d5s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
          else
            echo "Running full performance tests..."
            
            # å®Œæ•´æµ‹è¯•è¯´æ˜ï¼š
            # - æµ‹è¯•æ—¶é•¿ï¼š10ç§’ï¼ˆæä¾›è¶³å¤Ÿçš„ç»Ÿè®¡æ ·æœ¬ï¼Œç¡®ä¿ç»“æœå¯é ï¼‰
            # - æµ‹è¯•æ¬¡æ•°ï¼š3æ¬¡å–å¹³å‡ï¼ˆä½/ä¸­/é«˜å¹¶å‘ï¼‰ï¼Œ2æ¬¡å–å¹³å‡ï¼ˆå‹åŠ›/æ–‡ä»¶æµ‹è¯•ï¼‰
            # - ç»Ÿè®¡ä¾æ®ï¼š3æ¬¡æµ‹è¯•å¯æä¾› 95% ç½®ä¿¡åŒºé—´ï¼Œ2æ¬¡æµ‹è¯•ç”¨äºå¿«é€ŸéªŒè¯
            # - æ€»è€—æ—¶ï¼šçº¦10-15åˆ†é’Ÿï¼ˆåŒ…å«æ‰€æœ‰åœºæ™¯å’Œè¿­ä»£ï¼‰
            # - ç›®çš„ï¼šç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åŸºå‡†æŠ¥å‘Šï¼Œç”¨äºé•¿æœŸè¶‹åŠ¿åˆ†æ
            # ä½å¹¶å‘æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
            echo "Test 1: Low concurrency (10 connections, 10 seconds)" >> /tmp/performance.log
            for i in {1..3}; do
              wrk -t2 -c10 -d10s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
              sleep 2
            done
            
            # ä¸­ç­‰å¹¶å‘æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
            echo "" >> /tmp/performance.log
            echo "Test 2: Medium concurrency (50 connections, 10 seconds)" >> /tmp/performance.log
            for i in {1..3}; do
              wrk -t4 -c50 -d10s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
              sleep 2
            done
            
            # é«˜å¹¶å‘æµ‹è¯•ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
            echo "" >> /tmp/performance.log
            echo "Test 3: High concurrency (200 connections, 10 seconds)" >> /tmp/performance.log
            for i in {1..3}; do
              wrk -t8 -c200 -d10s --timeout 5s http://localhost:18081/ >> /tmp/performance.log 2>&1
              sleep 2
            done
          fi
          
          # æ¸…ç†
          kill $SERVER_PID 2>/dev/null || true
          wait $SERVER_PID 2>/dev/null || true
          
          # å¤åˆ¶æ—¥å¿—
          cp /tmp/performance.log ../performance.log
          cp /tmp/server.log ../server.log
        else
          echo "benchmark_rps not found"
          exit 1
        fi
    
    - name: Parse performance results
      id: parse
      env:
        TIMESTAMP: ${{ steps.date.outputs.timestamp }}
        RUN_ID: ${{ github.run_id }}
        RUN_NUMBER: ${{ github.run_number }}
        SHA: ${{ github.sha }}
        REF_NAME: ${{ github.ref_name }}
        # ä½¿ç”¨ fallback è¯­æ³•å¤„ç†å®šæ—¶ä»»åŠ¡ï¼ˆscheduleï¼‰ä¸­ head_commit ä¸ºç©ºçš„æƒ…å†µ
        COMMIT_MESSAGE: ${{ github.event.head_commit.message || github.event.commits[0].message || 'Scheduled run' }}
        COMMIT_AUTHOR: ${{ github.event.head_commit.author.name || github.event.commits[0].author.name || 'GitHub Actions' }}
      run: |
        python3 << 'EOF'
        import os
        import re
        import json
        from datetime import datetime
        
        # ä»ç¯å¢ƒå˜é‡è·å– GitHub Actions ä¸Šä¸‹æ–‡ï¼ˆæ·»åŠ è¾“å…¥éªŒè¯ï¼‰
        timestamp = os.environ.get('TIMESTAMP', 'unknown')
        
        # å®‰å…¨è½¬æ¢æ•´æ•°ï¼Œé¿å… ValueError
        try:
            run_id = int(os.environ.get('RUN_ID', 0))
        except (ValueError, TypeError):
            run_id = 0
        
        try:
            run_number = int(os.environ.get('RUN_NUMBER', 0))
        except (ValueError, TypeError):
            run_number = 0
        
        sha = os.environ.get('SHA', 'unknown')
        ref_name = os.environ.get('REF_NAME', 'unknown')
        commit_message = os.environ.get('COMMIT_MESSAGE', 'unknown')
        commit_author = os.environ.get('COMMIT_AUTHOR', 'unknown')
        
        # è¯»å–æ€§èƒ½æ—¥å¿—ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰
        try:
            with open('performance.log', 'r') as f:
                log_content = f.read()
        except FileNotFoundError:
            print("Warning: performance.log not found, using empty content")
            log_content = ""
        
        # è§£æ wrk è¾“å‡º
        def parse_wrk_output(text):
            match = re.search(r'(\d+)\s+requests in\s+([\d.]+)s', text)
            if not match:
                return None
            
            requests = int(match.group(1))
            duration = float(match.group(2))
            rps = requests / duration
            
            # è§£æå»¶è¿Ÿ
            latency_match = re.search(r'Latency\s+([\d.]+[a-zÂµ]+)\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+\s+([\d.]+)[a-zÂµ]+', text)
            if latency_match:
                latency_avg = latency_match.group(1)
                latency_stdev = latency_match.group(2)
                latency_p50 = latency_match.group(3)
                latency_p75 = latency_match.group(4)
                latency_p99 = latency_match.group(5)
            else:
                latency_avg = latency_stdev = latency_p50 = latency_p75 = latency_p99 = "N/A"
            
            # è§£æé”™è¯¯
            errors_match = re.search(r'(\d+)\s+errors', text)
            errors = int(errors_match.group(1)) if errors_match else 0
            
            # è§£æä¼ è¾“é€Ÿç‡
            transfer_match = re.search(r'([\d.]+)\s+([KMG]B/s)', text)
            if transfer_match:
                transfer_rate = transfer_match.group(1) + ' ' + transfer_match.group(2)
            else:
                transfer_rate = "N/A"
            
            return {
                'rps': rps,
                'latency_avg': latency_avg,
                'latency_stdev': latency_stdev,
                'latency_p50': latency_p50,
                'latency_p75': latency_p75,
                'latency_p99': latency_p99,
                'errors': errors,
                'transfer_rate': transfer_rate
            }
        
        # æå–å„ä¸ªæµ‹è¯•åœºæ™¯çš„ç»“æœ
        test_scenarios = [
            ('low_concurrent', 'Test 1: Low concurrency'),
            ('medium_concurrent', 'Test 2: Medium concurrency'),
            ('high_concurrent', 'Test 3: High concurrency')
        ]
        
        results = {}
        for scenario_name, test_name in test_scenarios:
            pattern = re.escape(test_name) + r'.*?((?:Requests/sec|Running|Requests in).*?)(?=\n\n|\nTest |\Z)'
            matches = re.findall(pattern, log_content, re.DOTALL)
            
            if matches:
                parsed_results = [parse_wrk_output(m) for m in matches if parse_wrk_output(m)]
                if parsed_results:
                    results[scenario_name] = parsed_results
        
        # ç”Ÿæˆ JSON ç»“æœ
        output = {
            'test_id': f'PERF-{timestamp}',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'run_id': run_id,
            'run_number': run_number,
            'commit': {
                'sha': sha,
                'short_sha': sha[:7] if len(sha) >= 7 else sha,
                'message': commit_message,
                'author': commit_author,
                'branch': ref_name
            },
            'environment': {
                'os': 'Linux 6.14.11-2-pve',
                'runner': 'ubuntu-22.04',
                'cpu': 'AMD Ryzen 7 5800H',
                'cores': 16,
                'memory': '12GB',
                'compiler': 'GCC',
                'compiler_version': '11.4.0',
                'build_type': 'Release',
                'optimization': '-O2'
            },
            'test_scenarios': [],
            'summary': {
                'total_scenarios': len(results),
                'passed': len(results),
                'warning': 0,
                'failed': 0,
                'overall_status': 'pass'
            }
        }
        
        # æ·»åŠ æµ‹è¯•åœºæ™¯ç»“æœ
        for scenario_name, scenario_results in results.items():
            # è®¡ç®—å¹³å‡å€¼
            avg_rps = sum(r['rps'] for r in scenario_results) / len(scenario_results)
            
            output['test_scenarios'].append({
                'name': scenario_name,
                'iterations': len(scenario_results),
                'results': {
                    'rps': {
                        'value': avg_rps,
                        'unit': 'req/s'
                    },
                    'latency_avg': {
                        'value': scenario_results[0]['latency_avg']
                    }
                }
            })
        
        # ä¿å­˜ JSON
        with open('performance-results.json', 'w') as f:
            json.dump(output, f, indent=2)
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡ï¼ˆæ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼‰
        if len(output['test_scenarios']) >= 3:
            print(f"rps_low={output['test_scenarios'][0]['results']['rps']['value']:.0f}")
            print(f"rps_medium={output['test_scenarios'][1]['results']['rps']['value']:.0f}")
            print(f"rps_high={output['test_scenarios'][2]['results']['rps']['value']:.0f}")
            print("status=pass")
        else:
            print(f"Error: Expected at least 3 test scenarios, got {len(output['test_scenarios'])}")
            print("status=error")
        
        EOF
        
        # å°†è¾“å‡ºä¿å­˜ä¸ºç¯å¢ƒå˜é‡ï¼ˆæ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼‰
        SCENARIO_COUNT=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(len(d['test_scenarios']))")
        
        if [ "$SCENARIO_COUNT" -ge 3 ]; then
          echo "rps_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][0]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
          echo "rps_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][1]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
          echo "rps_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(f\"{d['test_scenarios'][2]['results']['rps']['value']:.0f}\")")" >> $GITHUB_OUTPUT
          echo "latency_avg_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][0]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
          echo "latency_avg_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][1]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
          echo "latency_avg_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][2]['results']['latency_avg']['value'])")" >> $GITHUB_OUTPUT
          echo "latency_p99_low=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][0]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
          echo "latency_p99_medium=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][1]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
          echo "latency_p99_high=$(python3 -c "import json; d=json.load(open('performance-results.json')); print(d['test_scenarios'][2]['results'].get('latency_p99', 'N/A'))")" >> $GITHUB_OUTPUT
          echo "status=pass" >> $GITHUB_OUTPUT
        else
          echo "Error: Expected at least 3 test scenarios, got $SCENARIO_COUNT" >> $GITHUB_OUTPUT
          echo "status=error" >> $GITHUB_OUTPUT
        fi
    
    - name: Generate performance report
      env:
        TEST_DATE: ${{ steps.date.outputs.date }}
        TEST_DATETIME: ${{ steps.date.outputs.datetime }}
      run: |
        cat > /tmp/generate_report.py << 'SCRIPT_EOF'
        import os
        import json
        
        # ä»ç¯å¢ƒå˜é‡è·å–æ—¥æœŸä¿¡æ¯
        test_date = os.environ.get('TEST_DATE', 'unknown')
        test_datetime = os.environ.get('TEST_DATETIME', 'unknown')
        
        with open('performance-results.json', 'r') as f:
            data = json.load(f)
        
        report = "# UVHTTP æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n"
        report += "## æµ‹è¯•æ¦‚è¿°\n\n"
        report += "- **æµ‹è¯•æ—¥æœŸ**: " + test_date + "\n"
        report += "- **æµ‹è¯•æ—¶é—´**: " + test_datetime + "\n"
        report += "- **æµ‹è¯•ç‰ˆæœ¬**: v2.0.0\n"
        report += "- **æäº¤**: " + data['commit']['short_sha'] + " - " + data['commit']['message'] + "\n"
        report += "- **åˆ†æ”¯**: " + data['commit']['branch'] + "\n"
        report += "- **è¿è¡Œç¼–å·**: #" + str(data['run_number']) + "\n"
        report += "- **è¿è¡ŒID**: " + str(data['run_id']) + "\n"
        report += "\n"
        report += "## æµ‹è¯•ç¯å¢ƒ\n\n"
        report += "- **æ“ä½œç³»ç»Ÿ**: " + data['environment']['os'] + "\n"
        report += "- **Runner**: " + data['environment']['runner'] + "\n"
        report += "- **CPU**: " + data['environment']['cpu'] + " (" + str(data['environment']['cores']) + "æ ¸)\n"
        report += "- **å†…å­˜**: " + data['environment']['memory'] + "\n"
        report += "- **ç¼–è¯‘å™¨**: " + data['environment']['compiler'] + " " + data['environment']['compiler_version'] + "\n"
        report += "- **æ„å»ºç±»å‹**: " + data['environment']['build_type'] + "\n"
        report += "- **ä¼˜åŒ–çº§åˆ«**: " + data['environment']['optimization'] + "\n"
        report += "\n"
        report += "## æµ‹è¯•ç»“æœæ‘˜è¦\n\n"
        report += "| æŒ‡æ ‡ | é€šè¿‡ | è­¦å‘Š | å¤±è´¥ | æ€»è®¡ |\n"
        report += "|-----|-----|-----|-----|-----|\n"
        report += "| æµ‹è¯•åœºæ™¯ | " + str(data['summary']['passed']) + " | " + str(data['summary']['warning']) + " | " + str(data['summary']['failed']) + " | " + str(data['summary']['total_scenarios']) + " |\n"
        report += "\n"
        report += "**æ€»ä½“çŠ¶æ€**: âœ… é€šè¿‡\n"
        report += "\n"
        report += "## è¯¦ç»†æµ‹è¯•ç»“æœ\n\n"
        
        for i, scenario in enumerate(data['test_scenarios'], 1):
            report += "### åœºæ™¯ " + str(i) + ": " + scenario['name'] + "\n\n"
            report += "| æŒ‡æ ‡ | å½“å‰å€¼ |\n"
            report += "|-----|-------|\n"
            report += "| ååé‡ (RPS) | " + str(round(scenario['results']['rps']['value'])) + " req/s |\n"
            report += "| å¹³å‡å»¶è¿Ÿ | " + scenario['results']['latency_avg']['value'] + " |\n"
            report += "\n"
            report += "**æµ‹è¯•æ¬¡æ•°**: " + str(scenario['iterations']) + "\n\n"
        
        report += "## é™„å½•\n\n"
        report += "### æµ‹è¯•å‘½ä»¤\n\n"
        report += "```bash\n"
        report += "# ä½å¹¶å‘æµ‹è¯•\n"
        report += "wrk -t2 -c10 -d10s --timeout 5s http://127.0.0.1:18081/\n\n"
        report += "# ä¸­ç­‰å¹¶å‘æµ‹è¯•\n"
        report += "wrk -t4 -c50 -d10s --timeout 5s http://127.0.0.1:18081/\n\n"
        report += "# é«˜å¹¶å‘æµ‹è¯•\n"
        report += "wrk -t8 -c200 -d10s --timeout 5s http://127.0.0.1:18081/\n\n"
        report += "# æç«¯å¹¶å‘æµ‹è¯•ï¼ˆnightlyï¼‰\n"
        report += "wrk -t8 -c500 -d30s --timeout 10s http://127.0.0.1:18081/\n\n"
        report += "# è¶…é«˜å¹¶å‘æµ‹è¯•ï¼ˆnightlyï¼‰\n"
        report += "wrk -t8 -c1000 -d30s --timeout 10s http://127.0.0.1:18081/\n\n"
        report += "# æŒç»­è´Ÿè½½æµ‹è¯•ï¼ˆnightlyï¼‰\n"
        report += "wrk -t4 -c100 -d60s --timeout 5s http://127.0.0.1:18081/\n\n"
        report += "# å°æ–‡ä»¶ä¼ è¾“æµ‹è¯•ï¼ˆnightlyï¼‰\n"
        report += "wrk -t4 -c100 -d10s --timeout 5s http://127.0.0.1:18081/\n\n"
        report += "# å¤§æ–‡ä»¶ä¼ è¾“æµ‹è¯•ï¼ˆnightlyï¼‰\n"
        report += "wrk -t4 -c50 -d10s --timeout 10s http://127.0.0.1:18081/\n"
        report += "```\n\n"
        report += "---\n\n"
        report += "**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: " + data['timestamp'] + "\n"
        report += "**ç”Ÿæˆå·¥å…·**: GitHub Actions\n"
        report += "**æŠ¥å‘Šç‰ˆæœ¬**: 1.0.0\n"
        
        with open('performance-report.md', 'w') as f:
            f.write(report)
        
        print("Performance report generated successfully")
        SCRIPT_EOF
        
        python3 /tmp/generate_report.py
    
    - name: Upload performance results artifact
      uses: actions/upload-artifact@v4.3.1
      with:
        name: performance-results-${{ github.run_number }}
        path: |
          performance-results.json
          performance-report.md
          performance.log
          server.log
          baseline-history.json
        retention-days: 30
    
    - name: Create performance benchmark release
      if: github.event_name != 'pull_request'
      uses: softprops/action-gh-release@v1
      with:
        tag_name: perf-${{ steps.date.outputs.timestamp }}
        name: Performance Benchmark - ${{ steps.date.outputs.date }}
        body_path: performance-report.md
        draft: false
        prerelease: false
        files: |
          performance-results.json
          performance-report.md
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Create performance summary
      run: |
        echo "## ğŸ“Š Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Scenario | RPS | Avg Latency | P99 Latency | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-----|-------------|-------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Low Concurrent (10) | ${{ steps.parse.outputs.rps_low }} | ${{ steps.parse.outputs.latency_avg_low }} | ${{ steps.parse.outputs.latency_p99_low }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| Medium Concurrent (50) | ${{ steps.parse.outputs.rps_medium }} | ${{ steps.parse.outputs.latency_avg_medium }} | ${{ steps.parse.outputs.latency_p99_medium }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| High Concurrent (100) | ${{ steps.parse.outputs.rps_high }} | ${{ steps.parse.outputs.latency_avg_high }} | ${{ steps.parse.outputs.latency_p99_high }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Detailed Report" >> $GITHUB_STEP_SUMMARY
        echo "See [performance-report.md](https://github.com/${{ github.repository }}/releases/download/perf-${{ steps.date.outputs.timestamp }}/performance-report.md) for more details." >> $GITHUB_STEP_SUMMARY
    
    - name: Create baseline history
      if: github.event_name != 'pull_request'
      run: |
        # åˆ›å»ºåŸºå‡†æ€§èƒ½å†å²æ–‡ä»¶ï¼ˆä»…ç”¨äºäº§ç‰©ï¼‰
        HISTORY_FILE="baseline-history.json"
        
        # å¦‚æœå†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºæ•°ç»„
        if [ ! -f "$HISTORY_FILE" ]; then
          echo '[]' > "$HISTORY_FILE"
        fi
        
        # ä½¿ç”¨ jq æ›´æ–°å†å²æ–‡ä»¶
        jq --argjson current "$(cat performance-results.json)" '. + [$current] | .[:30]' "$HISTORY_FILE" > "${HISTORY_FILE}.tmp" && mv "${HISTORY_FILE}.tmp" "$HISTORY_FILE"
    
    - name: Save baseline to project
      if: github.event_name != 'pull_request' && startsWith(github.ref, 'refs/heads/release/')
      run: |
        # å°†åŸºå‡†æ€§èƒ½å†å²è®°å½•åˆ°å·¥ç¨‹æ–‡ä»¶ï¼ˆä»… release åˆ†æ”¯ï¼‰
        HISTORY_FILE="docs/performance/baseline-history.json"
        HISTORY_DIR=$(dirname "$HISTORY_FILE")
        
        # åˆ›å»ºç›®å½•
        mkdir -p "$HISTORY_DIR"
        
        # å¦‚æœå†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºæ•°ç»„
        if [ ! -f "$HISTORY_FILE" ]; then
          echo '[]' > "$HISTORY_FILE"
        fi
        
        # ä½¿ç”¨ jq æ›´æ–°å†å²æ–‡ä»¶
        jq --argjson current "$(cat performance-results.json)" '. + [$current] | .[:30]' "$HISTORY_FILE" > "${HISTORY_FILE}.tmp" && mv "${HISTORY_FILE}.tmp" "$HISTORY_FILE"
        
        # æäº¤å†å²æ–‡ä»¶æ›´æ–°
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add "$HISTORY_FILE"
        git diff --staged --quiet || git commit -m "chore: æ›´æ–°åŸºå‡†æ€§èƒ½å†å²è®°å½• [skip ci]"
        git push origin release